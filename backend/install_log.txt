Using pip 25.0.1 from /home/elicer/.local/lib/python3.10/site-packages/pip (python 3.10)
Defaulting to user installation because normal site-packages is not writeable
Collecting llama-cpp-python
  Downloading llama_cpp_python-0.3.8.tar.gz (67.3 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 67.3/67.3 MB 55.7 MB/s eta 0:00:00
  Installing build dependencies: started
  Running command pip subprocess to install build dependencies
  Using pip 25.0.1 from /home/elicer/.local/lib/python3.10/site-packages/pip (python 3.10)
  Collecting scikit-build-core>=0.9.2 (from scikit-build-core[pyproject]>=0.9.2)
    Obtaining dependency information for scikit-build-core>=0.9.2 from https://files.pythonhosted.org/packages/0a/ba/b37b9802f503894a46ef34aaa5851344cde48b39ab0af5057a6ee4f0d631/scikit_build_core-0.11.0-py3-none-any.whl.metadata
    Using cached scikit_build_core-0.11.0-py3-none-any.whl.metadata (21 kB)
  Collecting exceptiongroup>=1.0 (from scikit-build-core>=0.9.2->scikit-build-core[pyproject]>=0.9.2)
    Obtaining dependency information for exceptiongroup>=1.0 from https://files.pythonhosted.org/packages/02/cc/b7e31358aac6ed1ef2bb790a9746ac2c69bcb3c8588b41616914eb106eaf/exceptiongroup-1.2.2-py3-none-any.whl.metadata
    Using cached exceptiongroup-1.2.2-py3-none-any.whl.metadata (6.6 kB)
  Collecting packaging>=21.3 (from scikit-build-core>=0.9.2->scikit-build-core[pyproject]>=0.9.2)
    Obtaining dependency information for packaging>=21.3 from https://files.pythonhosted.org/packages/88/ef/eb23f262cca3c0c4eb7ab1933c3b1f03d021f2c48f54763065b6f0e321be/packaging-24.2-py3-none-any.whl.metadata
    Using cached packaging-24.2-py3-none-any.whl.metadata (3.2 kB)
  Collecting pathspec>=0.10.1 (from scikit-build-core>=0.9.2->scikit-build-core[pyproject]>=0.9.2)
    Obtaining dependency information for pathspec>=0.10.1 from https://files.pythonhosted.org/packages/cc/20/ff623b09d963f88bfde16306a54e12ee5ea43e9b597108672ff3a408aad6/pathspec-0.12.1-py3-none-any.whl.metadata
    Using cached pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)
  Collecting tomli>=1.2.2 (from scikit-build-core>=0.9.2->scikit-build-core[pyproject]>=0.9.2)
    Obtaining dependency information for tomli>=1.2.2 from https://files.pythonhosted.org/packages/6e/c2/61d3e0f47e2b74ef40a68b9e6ad5984f6241a942f7cd3bbfbdbd03861ea9/tomli-2.2.1-py3-none-any.whl.metadata
    Using cached tomli-2.2.1-py3-none-any.whl.metadata (10 kB)
  Using cached scikit_build_core-0.11.0-py3-none-any.whl (179 kB)
  Using cached exceptiongroup-1.2.2-py3-none-any.whl (16 kB)
  Using cached packaging-24.2-py3-none-any.whl (65 kB)
  Using cached pathspec-0.12.1-py3-none-any.whl (31 kB)
  Using cached tomli-2.2.1-py3-none-any.whl (14 kB)
  Installing collected packages: tomli, pathspec, packaging, exceptiongroup, scikit-build-core
  Successfully installed exceptiongroup-1.2.2 packaging-24.2 pathspec-0.12.1 scikit-build-core-0.11.0 tomli-2.2.1
  Installing build dependencies: finished with status 'done'
  Getting requirements to build wheel: started
  Running command Getting requirements to build wheel
  Getting requirements to build wheel: finished with status 'done'
  Preparing metadata (pyproject.toml): started
  Running command Preparing metadata (pyproject.toml)
  *** scikit-build-core 0.11.0 using CMake 3.22.1 (metadata_wheel)
  Preparing metadata (pyproject.toml): finished with status 'done'
Collecting typing-extensions>=4.5.0 (from llama-cpp-python)
  Obtaining dependency information for typing-extensions>=4.5.0 from https://files.pythonhosted.org/packages/26/9f/ad63fc0248c5379346306f8668cda6e2e2e9c95e01216d2b8ffd9ff037d0/typing_extensions-4.12.2-py3-none-any.whl.metadata
  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)
Collecting numpy>=1.20.0 (from llama-cpp-python)
  Obtaining dependency information for numpy>=1.20.0 from https://files.pythonhosted.org/packages/c2/07/2e5cc71193e3ef3a219ffcf6ca4858e46ea2be09c026ddd480d596b32867/numpy-2.2.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata
  Downloading numpy-2.2.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)
Collecting diskcache>=5.6.1 (from llama-cpp-python)
  Obtaining dependency information for diskcache>=5.6.1 from https://files.pythonhosted.org/packages/3f/27/4570e78fc0bf5ea0ca45eb1de3818a23787af9b390c0b0a0033a1b8236f9/diskcache-5.6.3-py3-none-any.whl.metadata
  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)
Collecting jinja2>=2.11.3 (from llama-cpp-python)
  Obtaining dependency information for jinja2>=2.11.3 from https://files.pythonhosted.org/packages/62/a1/3d680cbfd5f4b8f15abc1d571870c5fc3e594bb582bc3b64ea099db13e56/jinja2-3.1.6-py3-none-any.whl.metadata
  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)
Collecting MarkupSafe>=2.0 (from jinja2>=2.11.3->llama-cpp-python)
  Obtaining dependency information for MarkupSafe>=2.0 from https://files.pythonhosted.org/packages/22/35/137da042dfb4720b638d2937c38a9c2df83fe32d20e8c8f3185dbfef05f7/MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata
  Downloading MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)
Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)
Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)
Downloading numpy-2.2.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.4/16.4 MB 64.7 MB/s eta 0:00:00
Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)
Downloading MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20 kB)
Building wheels for collected packages: llama-cpp-python
  Building wheel for llama-cpp-python (pyproject.toml): started
  Running command Building wheel for llama-cpp-python (pyproject.toml)
  *** scikit-build-core 0.11.0 using CMake 3.22.1 (wheel)
  *** Configuring CMake...
  loading initial cache file /tmp/tmp11b93acy/build/CMakeInit.txt
  -- The C compiler identification is GNU 11.4.0
  -- The CXX compiler identification is GNU 11.4.0
  -- Detecting C compiler ABI info
  -- Detecting C compiler ABI info - done
  -- Check for working C compiler: /usr/bin/gcc - skipped
  -- Detecting C compile features
  -- Detecting C compile features - done
  -- Detecting CXX compiler ABI info
  -- Detecting CXX compiler ABI info - done
  -- Check for working CXX compiler: /usr/bin/g++ - skipped
  -- Detecting CXX compile features
  -- Detecting CXX compile features - done
  -- Found Git: /usr/bin/git (found version "2.34.1")
  -- Looking for pthread.h
  -- Looking for pthread.h - found
  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD
  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
  -- Found Threads: TRUE
  -- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF
  -- CMAKE_SYSTEM_PROCESSOR: x86_64
  -- Including CPU backend
  -- Found OpenMP_C: -fopenmp (found version "4.5")
  -- Found OpenMP_CXX: -fopenmp (found version "4.5")
  -- Found OpenMP: TRUE (found version "4.5")
  -- x86 detected
  -- Adding CPU backend variant ggml-cpu: -march=native
  -- Found CUDAToolkit: /usr/local/cuda-12.4/include (found version "12.4.131")
  -- CUDA Toolkit found
  -- Using CUDA architectures: 50;61;70;75;80
  -- The CUDA compiler identification is NVIDIA 12.4.131
  -- Detecting CUDA compiler ABI info
  -- Detecting CUDA compiler ABI info - done
  -- Check for working CUDA compiler: /usr/local/cuda-12.4/bin/nvcc - skipped
  -- Detecting CUDA compile features
  -- Detecting CUDA compile features - done
  -- CUDA host compiler is GNU 11.4.0

  -- Including CUDA backend
  CMake Warning at vendor/llama.cpp/ggml/CMakeLists.txt:298 (message):
    GGML build version fixed at 1 likely due to a shallow clone.


  INSTALL TARGETS - target llama has PUBLIC_HEADER files but no PUBLIC_HEADER DESTINATION.
  INSTALL TARGETS - target llama has PUBLIC_HEADER files but no PUBLIC_HEADER DESTINATION.
  INSTALL TARGETS - target ggml has PUBLIC_HEADER files but no PUBLIC_HEADER DESTINATION.
  INSTALL TARGETS - target ggml has PUBLIC_HEADER files but no PUBLIC_HEADER DESTINATION.
  -- Configuring done
  -- Generating done
  -- Build files have been written to: /tmp/tmp11b93acy/build
  *** Building project with Ninja...
  [1/150] /usr/bin/g++ -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_AARCH64 -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -std=gnu++17 -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cpu/ggml-cpu-hbm.cpp
  [2/150] /usr/bin/g++ -DGGML_BUILD -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_base_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -std=gnu++17 -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-threading.cpp
  [3/150] /usr/bin/gcc -DGGML_BUILD -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_base_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -std=gnu11 -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-alloc.c
  [4/150] /usr/bin/g++ -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_AARCH64 -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -std=gnu++17 -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cpu/ggml-cpu-traits.cpp
  [5/150] /usr/bin/g++ -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_AARCH64 -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -std=gnu++17 -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cpu/amx/mmq.cpp
  [6/150] /usr/bin/g++ -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_AARCH64 -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -std=gnu++17 -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cpu/amx/amx.cpp
  [7/150] /usr/bin/g++ -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_AARCH64 -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -std=gnu++17 -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cpu/ggml-cpu.cpp
  [8/150] /usr/bin/gcc -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_AARCH64 -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -march=native -fopenmp -std=gnu11 -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cpu/ggml-cpu-quants.c
  [9/150] /usr/bin/g++ -DGGML_BUILD -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_base_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -std=gnu++17 -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-opt.cpp
  [10/150] /usr/bin/g++ -DGGML_BUILD -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_base_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -std=gnu++17 -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-backend.cpp
  [11/150] /usr/bin/g++ -DGGML_BACKEND_SHARED -DGGML_BUILD -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -std=gnu++17 -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-backend-reg.cpp
  [12/150] /usr/bin/gcc -DGGML_BUILD -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_base_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -std=gnu11 -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml.c
  [13/150] /usr/bin/g++ -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_AARCH64 -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -std=gnu++17 -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cpu/ggml-cpu-aarch64.cpp
  [14/150] /usr/bin/g++ -DGGML_BUILD -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_base_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -std=gnu++17 -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/gguf.cpp
  [15/150] /usr/bin/g++ -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_AARCH64 -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -std=gnu++17 -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cpu/llamafile/sgemm.cpp
  [16/150] /usr/bin/gcc -DGGML_BUILD -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_base_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -std=gnu11 -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-quants.c
  [17/150] /usr/bin/gcc -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_AARCH64 -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -march=native -fopenmp -std=gnu11 -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cpu/ggml-cpu.c
  [18/150] : && /usr/bin/g++ -fPIC -O3 -DNDEBUG   -shared -Wl,-soname,libggml-base.so -o bin/libggml-base.so vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o  -Wl,-rpath,"\$ORIGIN"  -lm && :
  [19/150] : && /usr/bin/g++ -fPIC -O3 -DNDEBUG   -shared -Wl,-soname,libggml-cpu.so -o bin/libggml-cpu.so vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o  -Wl,-rpath,"\$ORIGIN"  bin/libggml-base.so  /usr/lib/gcc/x86_64-linux-gnu/11/libgomp.so  /usr/lib/x86_64-linux-gnu/libpthread.a && :
  [20/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/argsort.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/argsort.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/argsort.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/argsort.cu.o
  [21/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/acc.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/acc.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/acc.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/acc.cu.o
  [22/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/arange.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/arange.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/arange.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/arange.cu.o
  [23/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/clamp.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/clamp.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/clamp.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/clamp.cu.o
  [24/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/concat.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/concat.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/concat.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/concat.cu.o
  [25/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv-transpose-1d.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv-transpose-1d.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/conv-transpose-1d.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv-transpose-1d.cu.o
  [26/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/argmax.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/argmax.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/argmax.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/argmax.cu.o
  [27/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/count-equal.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/count-equal.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/count-equal.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/count-equal.cu.o
  [28/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/diagmask.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/diagmask.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/diagmask.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/diagmask.cu.o
  [29/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cross-entropy-loss.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cross-entropy-loss.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/cross-entropy-loss.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cross-entropy-loss.cu.o
  [30/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/convert.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/convert.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/convert.cu.o
  [31/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cpy.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cpy.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/cpy.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cpy.cu.o
  [32/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/fattn.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn.cu.o
  [33/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/binbcast.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/binbcast.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/binbcast.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/binbcast.cu.o
  [34/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/getrows.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/getrows.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/getrows.cu.o
  [35/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/im2col.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/im2col.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/im2col.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/im2col.cu.o
  [36/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmq.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmq.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/mmq.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmq.cu.o
  [37/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/out-prod.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/out-prod.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/out-prod.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/out-prod.cu.o
  [38/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/gla.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/gla.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/gla.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/gla.cu.o
  [39/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/opt-step-adamw.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/opt-step-adamw.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/opt-step-adamw.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/opt-step-adamw.cu.o
  [40/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pad.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pad.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/pad.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pad.cu.o
  [41/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/norm.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/norm.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/norm.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/norm.cu.o
  [42/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pool2d.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pool2d.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/pool2d.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pool2d.cu.o
  [43/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ggml-cuda.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ggml-cuda.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/ggml-cuda.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ggml-cuda.cu.o
  [44/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-tile-f16.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-tile-f16.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/fattn-tile-f16.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-tile-f16.cu.o
  [45/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/quantize.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/quantize.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/quantize.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/quantize.cu.o
  [46/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/scale.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/scale.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/scale.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/scale.cu.o
  [47/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/sumrows.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/sumrows.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/sumrows.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/sumrows.cu.o
  [48/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/tsembd.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/tsembd.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/tsembd.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/tsembd.cu.o
  [49/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmv.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmv.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/mmv.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmv.cu.o
  [50/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/softmax.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/softmax.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/softmax.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/softmax.cu.o
  [51/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/rope.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/rope.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/rope.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/rope.cu.o
  [52/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/upscale.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/upscale.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/upscale.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/upscale.cu.o
  [53/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/unary.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/unary.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/unary.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/unary.cu.o
  [54/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/wkv6.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/wkv6.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/wkv6.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/wkv6.cu.o
  [55/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-tile-f32.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-tile-f32.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/fattn-tile-f32.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-tile-f32.cu.o
  [56/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/sum.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/sum.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/sum.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/sum.cu.o
  [57/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu.o
  [58/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_1.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_1.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_1.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_1.cu.o
  [59/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu.o
  [60/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu.o
  [61/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_1.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_1.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_1.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_1.cu.o
  [62/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_4.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_4.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_4.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_4.cu.o
  [63/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_2.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_2.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_2.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_2.cu.o
  [64/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_1.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_1.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_1.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_1.cu.o
  [65/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_8.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_8.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_8.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_8.cu.o
  [66/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_2.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_2.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_2.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_2.cu.o
  [67/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_8.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_8.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_8.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_8.cu.o
  [68/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_4.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_4.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_4.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_4.cu.o
  [69/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_2.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_2.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_2.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_2.cu.o
  [70/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_4.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_4.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_4.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_4.cu.o
  [71/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_64-ncols2_1.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_64-ncols2_1.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_64-ncols2_1.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_64-ncols2_1.cu.o
  [72/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_8.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_8.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_8.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_8.cu.o
  [73/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-wmma-f16.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-wmma-f16.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/fattn-wmma-f16.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-wmma-f16.cu.o
  [74/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmvq.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmvq.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/mmvq.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmvq.cu.o
  [75/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_xxs.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_xxs.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq2_xxs.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_xxs.cu.o
  [76/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq1_s.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq1_s.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq1_s.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq1_s.cu.o
  [77/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq3_xxs.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq3_xxs.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq3_xxs.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq3_xxs.cu.o
  [78/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq3_s.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq3_s.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq3_s.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq3_s.cu.o
  [79/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_xs.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_xs.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq2_xs.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_xs.cu.o
  [80/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_0.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_0.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q4_0.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_0.cu.o
  [81/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq4_nl.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq4_nl.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq4_nl.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq4_nl.cu.o
  [82/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q3_k.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q3_k.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q3_k.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q3_k.cu.o
  [83/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_s.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_s.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq2_s.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_s.cu.o
  [84/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_1.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_1.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q4_1.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_1.cu.o
  [85/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_1.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_1.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q5_1.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_1.cu.o
  [86/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq4_xs.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq4_xs.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq4_xs.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq4_xs.cu.o
  [87/150] /usr/bin/g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/../include -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/../common -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-adapter.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-adapter.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-adapter.cpp.o -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/llama-adapter.cpp
  [88/150] /usr/bin/g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/../include -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/../common -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama.cpp.o -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/llama.cpp
  [89/150] /usr/bin/g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/../include -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/../common -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-batch.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-batch.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-batch.cpp.o -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/llama-batch.cpp
  [90/150] /usr/bin/g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/../include -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/../common -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-arch.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-arch.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-arch.cpp.o -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/llama-arch.cpp
  [91/150] /usr/bin/g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/../include -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/../common -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-chat.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-chat.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-chat.cpp.o -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/llama-chat.cpp
  [92/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_k.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_k.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q4_k.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_k.cu.o
  [93/150] /usr/bin/g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/../include -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/../common -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-hparams.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-hparams.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-hparams.cpp.o -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/llama-hparams.cpp
  [94/150] /usr/bin/g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/../include -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/../common -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-impl.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-impl.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-impl.cpp.o -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/llama-impl.cpp
  [95/150] /usr/bin/g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/../include -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/../common -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-context.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-context.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-context.cpp.o -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/llama-context.cpp
  [96/150] /usr/bin/g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/../include -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/../common -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-mmap.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-mmap.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-mmap.cpp.o -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/llama-mmap.cpp
  [97/150] /usr/bin/g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/../include -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/../common -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/llama-kv-cache.cpp
  [98/150] /usr/bin/g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/../include -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/../common -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-model-loader.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-model-loader.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-model-loader.cpp.o -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/llama-model-loader.cpp
  [99/150] /usr/bin/g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/../include -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/../common -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-grammar.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-grammar.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-grammar.cpp.o -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/llama-grammar.cpp
  [100/150] /usr/bin/g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/../include -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/../common -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-quant.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-quant.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-quant.cpp.o -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/llama-quant.cpp
  [101/150] /usr/bin/g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/../include -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/../common -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-model.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-model.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-model.cpp.o -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/llama-model.cpp
  [102/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_0.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_0.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q5_0.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_0.cu.o
  [103/150] /usr/bin/g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/../include -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/../common -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/unicode-data.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/unicode-data.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/unicode-data.cpp.o -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/unicode-data.cpp
  [104/150] cd /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp && /usr/bin/cmake -DMSVC= -DCMAKE_C_COMPILER_VERSION=11.4.0 -DCMAKE_C_COMPILER_ID=GNU -DCMAKE_VS_PLATFORM_NAME= -DCMAKE_C_COMPILER=/usr/bin/gcc -P /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/common/cmake/build-info-gen-cpp.cmake
  -- Found Git: /usr/bin/git (found version "2.34.1")
  [105/150] /usr/bin/g++   -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.o -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/common/build-info.cpp
  [106/150] /usr/bin/g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/../include -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/../common -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-vocab.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-vocab.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-vocab.cpp.o -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/llama-vocab.cpp
  [107/150] /usr/bin/g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/../include -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/../common -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-sampling.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-sampling.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-sampling.cpp.o -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/llama-sampling.cpp
  [108/150] /usr/bin/g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/../include -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/../common -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/unicode.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/unicode.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/unicode.cpp.o -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/unicode.cpp
  [109/150] /usr/bin/g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_SHARED -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/common/. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/../include -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/../common -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/console.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/console.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/console.cpp.o -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/common/console.cpp
  [110/150] /usr/bin/g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_SHARED -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/common/. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/../include -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/../common -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/common.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/common.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/common.cpp.o -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/common/common.cpp
  [111/150] /usr/bin/g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_SHARED -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/common/. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/../include -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/../common -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/llguidance.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/llguidance.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/llguidance.cpp.o -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/common/llguidance.cpp
  [112/150] /usr/bin/g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_SHARED -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/common/. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/../include -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/../common -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/log.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/log.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/log.cpp.o -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/common/log.cpp
  [113/150] /usr/bin/g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_SHARED -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/common/. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/../include -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/../common -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/arg.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/arg.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/arg.cpp.o -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/common/arg.cpp
  [114/150] /usr/bin/g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_SHARED -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/common/. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/../include -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/../common -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/ngram-cache.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/ngram-cache.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/ngram-cache.cpp.o -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/common/ngram-cache.cpp
  [115/150] /usr/bin/g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_SHARED -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/common/. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/../include -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/../common -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/speculative.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/speculative.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/speculative.cpp.o -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/common/speculative.cpp
  [116/150] /usr/bin/g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/examples/llava/. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/examples/llava/../.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/examples/llava/../../common -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/include -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/include -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/../include -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/../common -O3 -DNDEBUG -fPIC -Wno-cast-qual -MD -MT vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o -MF vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o.d -o vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/examples/llava/llava.cpp
  [117/150] /usr/bin/g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_SHARED -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/common/. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/../include -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/../common -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/sampling.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/sampling.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/sampling.cpp.o -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/common/sampling.cpp
  [118/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_k.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_k.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q5_k.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_k.cu.o
  [119/150] /usr/bin/g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_SHARED -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/common/. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/../include -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/../common -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/common/json-schema-to-grammar.cpp
  [120/150] /usr/bin/g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_SHARED -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/include -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/common/. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/../include -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/../common -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/examples/llava/. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/examples/llava/../.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/examples/llava/../../common -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/include -O3 -DNDEBUG -MD -MT vendor/llama.cpp/examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o -MF vendor/llama.cpp/examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o.d -o vendor/llama.cpp/examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/examples/llava/llava-cli.cpp
  [121/150] /usr/bin/g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_SHARED -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/include -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/common/. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/../include -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/../common -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/examples/llava/. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/examples/llava/../.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/examples/llava/../../common -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/include -O3 -DNDEBUG -MD -MT vendor/llama.cpp/examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o -MF vendor/llama.cpp/examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o.d -o vendor/llama.cpp/examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/examples/llava/minicpmv-cli.cpp
  [122/150] /usr/bin/g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_SHARED -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/common/. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/../include -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/../common -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/examples/llava/. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/examples/llava/../.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/examples/llava/../../common -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/include -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/include -O3 -DNDEBUG -MD -MT vendor/llama.cpp/examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o -MF vendor/llama.cpp/examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o.d -o vendor/llama.cpp/examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/examples/llava/clip-quantize-cli.cpp
  [123/150] /usr/bin/g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_SHARED -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/common/. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/../include -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/../common -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/examples/llava/. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/examples/llava/../.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/examples/llava/../../common -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/include -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/include -O3 -DNDEBUG -MD -MT vendor/llama.cpp/examples/llava/CMakeFiles/llama-gemma3-cli.dir/gemma3-cli.cpp.o -MF vendor/llama.cpp/examples/llava/CMakeFiles/llama-gemma3-cli.dir/gemma3-cli.cpp.o.d -o vendor/llama.cpp/examples/llava/CMakeFiles/llama-gemma3-cli.dir/gemma3-cli.cpp.o -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/examples/llava/gemma3-cli.cpp
  [124/150] /usr/bin/g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_SHARED -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/common/. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/../include -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/../common -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/examples/llava/. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/examples/llava/../.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/examples/llava/../../common -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/include -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/include -O3 -DNDEBUG -MD -MT vendor/llama.cpp/examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o -MF vendor/llama.cpp/examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o.d -o vendor/llama.cpp/examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/examples/llava/qwen2vl-cli.cpp
  [125/150] /usr/bin/g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_SHARED -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/common/. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/../include -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/../common -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/chat.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/chat.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/chat.cpp.o -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/common/chat.cpp
  [126/150] /usr/bin/g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/examples/llava/. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/examples/llava/../.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/examples/llava/../../common -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/include -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/include -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/../include -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/src/../common -O3 -DNDEBUG -fPIC -Wno-cast-qual -MD -MT vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o -MF vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o.d -o vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/examples/llava/clip.cpp
  [127/150] : && /usr/bin/cmake -E rm -f vendor/llama.cpp/examples/llava/libllava_static.a && /usr/bin/ar qc vendor/llama.cpp/examples/llava/libllava_static.a  vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o && /usr/bin/ranlib vendor/llama.cpp/examples/llava/libllava_static.a && :
  [128/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs64-f16-f16.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs64-f16-f16.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs64-f16-f16.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs64-f16-f16.cu.o
  [129/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q8_0.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q8_0.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q8_0.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q8_0.cu.o
  [130/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs64-f16-f16.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs64-f16-f16.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs64-f16-f16.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs64-f16-f16.cu.o
  [131/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q2_k.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q2_k.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q2_k.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q2_k.cu.o
  [132/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q6_k.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q6_k.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q6_k.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q6_k.cu.o
  [133/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs128-q8_0-q8_0.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs128-q8_0-q8_0.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q8_0-q8_0.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs128-q8_0-q8_0.cu.o
  [134/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs128-q4_0-q4_0.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs128-q4_0-q4_0.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_0-q4_0.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs128-q4_0-q4_0.cu.o
  [135/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs128-f16-f16.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs128-f16-f16.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-f16-f16.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs128-f16-f16.cu.o
  [136/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs128-f16-f16.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs128-f16-f16.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-f16-f16.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs128-f16-f16.cu.o
  [137/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs128-q8_0-q8_0.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs128-q8_0-q8_0.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q8_0-q8_0.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs128-q8_0-q8_0.cu.o
  [138/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs128-q4_0-q4_0.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs128-q4_0-q4_0.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_0-q4_0.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs128-q4_0-q4_0.cu.o
  [139/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs256-f16-f16.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs256-f16-f16.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs256-f16-f16.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs256-f16-f16.cu.o
  [140/150] /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/../include -isystem=/usr/local/cuda-12.4/include -O3 -DNDEBUG --generate-code=arch=compute_50,code=[compute_50,sm_50] --generate-code=arch=compute_61,code=[compute_61,sm_61] --generate-code=arch=compute_70,code=[compute_70,sm_70] --generate-code=arch=compute_75,code=[compute_75,sm_75] --generate-code=arch=compute_80,code=[compute_80,sm_80] -Xcompiler=-fPIC -use_fast_math -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -std=c++17 -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs256-f16-f16.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs256-f16-f16.cu.o.d -x cu -c /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs256-f16-f16.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs256-f16-f16.cu.o
  [141/150] : && /usr/bin/g++ -fPIC  -shared -Wl,-soname,libggml-cuda.so -o bin/libggml-cuda.so vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/acc.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/arange.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/argmax.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/argsort.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/binbcast.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/clamp.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/concat.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv-transpose-1d.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/convert.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/count-equal.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cpy.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cross-entropy-loss.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/diagmask.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-tile-f16.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-tile-f32.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-wmma-f16.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/getrows.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ggml-cuda.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/gla.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/im2col.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmq.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmv.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmvq.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/norm.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/opt-step-adamw.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/out-prod.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pad.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pool2d.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/quantize.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/rope.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/scale.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/softmax.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/sum.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/sumrows.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/tsembd.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/unary.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/upscale.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/wkv6.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_1.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_2.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_4.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_8.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_1.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_2.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_4.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_8.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_64-ncols2_1.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_1.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_2.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_4.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_8.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq1_s.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_s.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_xs.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_xxs.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq3_s.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq3_xxs.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq4_nl.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq4_xs.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q2_k.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q3_k.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_0.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_1.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_k.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_0.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_1.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_k.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q6_k.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q8_0.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs128-q4_0-q4_0.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs128-q4_0-q4_0.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs128-q8_0-q8_0.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs128-q8_0-q8_0.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs128-f16-f16.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs256-f16-f16.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs64-f16-f16.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs128-f16-f16.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs256-f16-f16.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs64-f16-f16.cu.o  -Wl,-rpath,"\$ORIGIN"  bin/libggml-base.so  /usr/local/cuda-12.4/lib64/libcudart.so  /usr/local/cuda-12.4/lib64/libcublas.so  /usr/local/cuda-12.4/lib64/libcublasLt.so  /usr/lib/x86_64-linux-gnu/libcuda.so  -lcudadevrt  -lcudart_static  -lrt  -lpthread  -ldl -L"/usr/local/cuda-12.4/targets/x86_64-linux/lib/stubs" -L"/usr/local/cuda-12.4/targets/x86_64-linux/lib" && :
  [142/150] : && /usr/bin/g++ -fPIC -O3 -DNDEBUG   -shared -Wl,-soname,libggml.so -o bin/libggml.so vendor/llama.cpp/ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o  -Wl,-rpath,"\$ORIGIN"  -ldl  -lstdc++fs  bin/libggml-cpu.so  bin/libggml-cuda.so  bin/libggml-base.so && :
  [143/150] : && /usr/bin/g++ -fPIC -O3 -DNDEBUG   -shared -Wl,-soname,libllama.so -o bin/libllama.so vendor/llama.cpp/src/CMakeFiles/llama.dir/llama.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-adapter.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-arch.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-batch.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-chat.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-context.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-grammar.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-hparams.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-impl.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-mmap.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-model-loader.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-model.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-quant.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-sampling.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-vocab.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/unicode.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/unicode-data.cpp.o  -Wl,-rpath,"\$ORIGIN"  bin/libggml.so  bin/libggml-cpu.so  bin/libggml-cuda.so  bin/libggml-base.so && :
  [144/150] : && /usr/bin/cmake -E rm -f vendor/llama.cpp/common/libcommon.a && /usr/bin/ar qc vendor/llama.cpp/common/libcommon.a  vendor/llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/arg.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/chat.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/common.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/console.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/llguidance.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/log.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/ngram-cache.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/sampling.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/speculative.cpp.o && /usr/bin/ranlib vendor/llama.cpp/common/libcommon.a && :
  [145/150] : && /usr/bin/g++ -fPIC -O3 -DNDEBUG   -shared -Wl,-soname,libllava.so -o vendor/llama.cpp/examples/llava/libllava.so vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o  -Wl,-rpath,"\$ORIGIN"  bin/libllama.so  bin/libggml.so  bin/libggml-cpu.so  bin/libggml-cuda.so  bin/libggml-base.so && :
  [146/150] : && /usr/bin/g++ -O3 -DNDEBUG  vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o vendor/llama.cpp/examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o -o vendor/llama.cpp/examples/llava/llama-llava-clip-quantize-cli  -Wl,-rpath,/tmp/tmp11b93acy/build/bin:  vendor/llama.cpp/common/libcommon.a  bin/libllama.so  bin/libggml.so  bin/libggml-cpu.so  bin/libggml-cuda.so  bin/libggml-base.so && :
  [147/150] : && /usr/bin/g++ -O3 -DNDEBUG  vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o vendor/llama.cpp/examples/llava/CMakeFiles/llama-gemma3-cli.dir/gemma3-cli.cpp.o -o vendor/llama.cpp/examples/llava/llama-gemma3-cli  -Wl,-rpath,/tmp/tmp11b93acy/build/bin:  vendor/llama.cpp/common/libcommon.a  bin/libllama.so  bin/libggml.so  bin/libggml-cpu.so  bin/libggml-cuda.so  bin/libggml-base.so && :
  [148/150] : && /usr/bin/g++ -O3 -DNDEBUG  vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o vendor/llama.cpp/examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o -o vendor/llama.cpp/examples/llava/llama-qwen2vl-cli  -Wl,-rpath,/tmp/tmp11b93acy/build/bin:  vendor/llama.cpp/common/libcommon.a  bin/libllama.so  bin/libggml.so  bin/libggml-cpu.so  bin/libggml-cuda.so  bin/libggml-base.so && :
  [149/150] : && /usr/bin/g++ -O3 -DNDEBUG  vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o vendor/llama.cpp/examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o -o vendor/llama.cpp/examples/llava/llama-minicpmv-cli  -Wl,-rpath,/tmp/tmp11b93acy/build/bin:  vendor/llama.cpp/common/libcommon.a  bin/libllama.so  bin/libggml.so  bin/libggml-cpu.so  bin/libggml-cuda.so  bin/libggml-base.so && :
  [150/150] : && /usr/bin/g++ -O3 -DNDEBUG  vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o vendor/llama.cpp/examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o -o vendor/llama.cpp/examples/llava/llama-llava-cli  -Wl,-rpath,/tmp/tmp11b93acy/build/bin:  vendor/llama.cpp/common/libcommon.a  bin/libllama.so  bin/libggml.so  bin/libggml-cpu.so  bin/libggml-cuda.so  bin/libggml-base.so && :
  *** Installing project into wheel...
  -- Install configuration: "Release"
  -- Installing: /tmp/tmp11b93acy/wheel/platlib/lib/libggml-cpu.so
  -- Installing: /tmp/tmp11b93acy/wheel/platlib/lib/libggml-cuda.so
  -- Installing: /tmp/tmp11b93acy/wheel/platlib/lib/libggml.so
  -- Installing: /tmp/tmp11b93acy/wheel/platlib/include/ggml.h
  -- Installing: /tmp/tmp11b93acy/wheel/platlib/include/ggml-cpu.h
  -- Installing: /tmp/tmp11b93acy/wheel/platlib/include/ggml-alloc.h
  -- Installing: /tmp/tmp11b93acy/wheel/platlib/include/ggml-backend.h
  -- Installing: /tmp/tmp11b93acy/wheel/platlib/include/ggml-blas.h
  -- Installing: /tmp/tmp11b93acy/wheel/platlib/include/ggml-cann.h
  -- Installing: /tmp/tmp11b93acy/wheel/platlib/include/ggml-cpp.h
  -- Installing: /tmp/tmp11b93acy/wheel/platlib/include/ggml-cuda.h
  -- Installing: /tmp/tmp11b93acy/wheel/platlib/include/ggml-kompute.h
  -- Installing: /tmp/tmp11b93acy/wheel/platlib/include/ggml-opt.h
  -- Installing: /tmp/tmp11b93acy/wheel/platlib/include/ggml-metal.h
  -- Installing: /tmp/tmp11b93acy/wheel/platlib/include/ggml-rpc.h
  -- Installing: /tmp/tmp11b93acy/wheel/platlib/include/ggml-sycl.h
  -- Installing: /tmp/tmp11b93acy/wheel/platlib/include/ggml-vulkan.h
  -- Installing: /tmp/tmp11b93acy/wheel/platlib/include/gguf.h
  -- Installing: /tmp/tmp11b93acy/wheel/platlib/lib/libggml-base.so
  -- Installing: /tmp/tmp11b93acy/wheel/platlib/lib/cmake/ggml/ggml-config.cmake
  -- Installing: /tmp/tmp11b93acy/wheel/platlib/lib/cmake/ggml/ggml-version.cmake
  -- Installing: /tmp/tmp11b93acy/wheel/platlib/lib/libllama.so
  -- Installing: /tmp/tmp11b93acy/wheel/platlib/include/llama.h
  -- Installing: /tmp/tmp11b93acy/wheel/platlib/include/llama-cpp.h
  -- Installing: /tmp/tmp11b93acy/wheel/platlib/lib/cmake/llama/llama-config.cmake
  -- Installing: /tmp/tmp11b93acy/wheel/platlib/lib/cmake/llama/llama-version.cmake
  -- Installing: /tmp/tmp11b93acy/wheel/platlib/bin/convert_hf_to_gguf.py
  -- Installing: /tmp/tmp11b93acy/wheel/platlib/lib/pkgconfig/llama.pc
  -- Installing: /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/llama_cpp/lib/libllama.so
  -- Installing: /tmp/tmp11b93acy/wheel/platlib/llama_cpp/lib/libllama.so
  -- Installing: /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/llama_cpp/lib/libggml.so
  -- Installing: /tmp/tmp11b93acy/wheel/platlib/llama_cpp/lib/libggml.so
  -- Installing: /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/llama_cpp/lib/libggml-base.so
  -- Installing: /tmp/tmp11b93acy/wheel/platlib/llama_cpp/lib/libggml-base.so
  -- Installing: /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/llama_cpp/lib/libggml-cpu.so
  -- Installing: /tmp/tmp11b93acy/wheel/platlib/llama_cpp/lib/libggml-cpu.so
  -- Installing: /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/llama_cpp/lib/libggml-cuda.so
  -- Installing: /tmp/tmp11b93acy/wheel/platlib/llama_cpp/lib/libggml-cuda.so
  -- Installing: /tmp/tmp11b93acy/wheel/platlib/lib/libllava.so
  -- Installing: /tmp/tmp11b93acy/wheel/platlib/bin/llama-llava-cli
  -- Set runtime path of "/tmp/tmp11b93acy/wheel/platlib/bin/llama-llava-cli" to ""
  -- Installing: /tmp/tmp11b93acy/wheel/platlib/bin/llama-minicpmv-cli
  -- Set runtime path of "/tmp/tmp11b93acy/wheel/platlib/bin/llama-minicpmv-cli" to ""
  -- Installing: /tmp/tmp11b93acy/wheel/platlib/bin/llama-qwen2vl-cli
  -- Set runtime path of "/tmp/tmp11b93acy/wheel/platlib/bin/llama-qwen2vl-cli" to ""
  -- Installing: /tmp/tmp11b93acy/wheel/platlib/bin/llama-gemma3-cli
  -- Set runtime path of "/tmp/tmp11b93acy/wheel/platlib/bin/llama-gemma3-cli" to ""
  -- Installing: /tmp/tmp11b93acy/wheel/platlib/bin/llama-llava-clip-quantize-cli
  -- Set runtime path of "/tmp/tmp11b93acy/wheel/platlib/bin/llama-llava-clip-quantize-cli" to ""
  -- Installing: /tmp/pip-install-sxes58hc/llama-cpp-python_be389dc70a40415c84a245bd3ad1e42b/llama_cpp/lib/libllava.so
  -- Installing: /tmp/tmp11b93acy/wheel/platlib/llama_cpp/lib/libllava.so
  *** Making wheel...
  *** Created llama_cpp_python-0.3.8-cp310-cp310-linux_x86_64.whl
  Building wheel for llama-cpp-python (pyproject.toml): finished with status 'done'
  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.8-cp310-cp310-linux_x86_64.whl size=370125290 sha256=0730e0fa66c2e2486c55cc4baa633029ddee7aae19090ad54d05c5324ffb59c9
  Stored in directory: /tmp/pip-ephem-wheel-cache-hnmetfid/wheels/1e/7f/93/0ba01e12caa1597ca802c25d2445e4af62dbbb630bc05094b0
Successfully built llama-cpp-python
Installing collected packages: typing-extensions, numpy, MarkupSafe, diskcache, jinja2, llama-cpp-python
  Attempting uninstall: typing-extensions
    Found existing installation: typing_extensions 4.12.2
    Uninstalling typing_extensions-4.12.2:
      Removing file or directory /home/elicer/.local/lib/python3.10/site-packages/__pycache__/typing_extensions.cpython-310.pyc
      Removing file or directory /home/elicer/.local/lib/python3.10/site-packages/typing_extensions-4.12.2.dist-info/
      Removing file or directory /home/elicer/.local/lib/python3.10/site-packages/typing_extensions.py
      Successfully uninstalled typing_extensions-4.12.2
  Attempting uninstall: numpy
    Found existing installation: numpy 1.26.4
    Uninstalling numpy-1.26.4:
      Removing file or directory /home/elicer/.local/bin/f2py
      Removing file or directory /home/elicer/.local/lib/python3.10/site-packages/numpy-1.26.4.dist-info/
      Removing file or directory /home/elicer/.local/lib/python3.10/site-packages/numpy.libs/
      Removing file or directory /home/elicer/.local/lib/python3.10/site-packages/numpy/
      Successfully uninstalled numpy-1.26.4
  changing mode of /home/elicer/.local/bin/f2py to 775
  changing mode of /home/elicer/.local/bin/numpy-config to 775
  Attempting uninstall: MarkupSafe
    Found existing installation: MarkupSafe 3.0.2
    Uninstalling MarkupSafe-3.0.2:
      Removing file or directory /home/elicer/.local/lib/python3.10/site-packages/MarkupSafe-3.0.2.dist-info/
      Removing file or directory /home/elicer/.local/lib/python3.10/site-packages/markupsafe/
      Successfully uninstalled MarkupSafe-3.0.2
  Attempting uninstall: diskcache
    Found existing installation: diskcache 5.6.3
    Uninstalling diskcache-5.6.3:
      Removing file or directory /home/elicer/.local/lib/python3.10/site-packages/diskcache-5.6.3.dist-info/
      Removing file or directory /home/elicer/.local/lib/python3.10/site-packages/diskcache/
      Successfully uninstalled diskcache-5.6.3
  Attempting uninstall: jinja2
    Found existing installation: Jinja2 3.1.6
    Uninstalling Jinja2-3.1.6:
      Removing file or directory /home/elicer/.local/lib/python3.10/site-packages/jinja2-3.1.6.dist-info/
      Removing file or directory /home/elicer/.local/lib/python3.10/site-packages/jinja2/
      Successfully uninstalled Jinja2-3.1.6
ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.4 which is incompatible.
vllm 0.8.0 requires numpy<2.0.0, but you have numpy 2.2.4 which is incompatible.
Successfully installed MarkupSafe-3.0.2 diskcache-5.6.3 jinja2-3.1.6 llama-cpp-python-0.3.8 numpy-2.2.4 typing-extensions-4.12.2
